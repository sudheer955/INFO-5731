{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca22646-cb3f-4b45-a582-b42750159e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: ['is', 'the', 'captivating', 'storyline', 'absolutely', 'fantastic', 'superb', 'this', 'acting', 'and']\n",
            "Topic 2: ['was', 'it', 'the', 'terrible', 'stand', 'film', 'weak', 'plot', 'couldn', 'some']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.matutils import Sparse2Corpus\n",
        "\n",
        "movie_reviews = [\n",
        "    \"This movie is absolutely fantastic! The acting is superb and the storyline is captivating.\",\n",
        "    \"I couldn't stand this film. The plot was weak, and the acting was terrible.\",\n",
        "    \"The movie was okay. It had some good moments but overall, it was disappointing.\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(movie_reviews)\n",
        "\n",
        "\n",
        "corpus = Sparse2Corpus(X.T)\n",
        "\n",
        "\n",
        "coherence_scores = []\n",
        "for k in range(2, 11): # You can adjust the range for K\n",
        "   lda_model = LatentDirichletAllocation(n_components=k, random_state=42)\n",
        "   lda_model.fit(X)\n",
        "   id2word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
        "   lda_gensim = LdaModel(\n",
        "       corpus,\n",
        "       num_topics=k,\n",
        "       id2word=id2word,\n",
        "       passes=15,\n",
        "       iterations=100,\n",
        "       random_state=42\n",
        "   )\n",
        "   coherence_model_lda = CoherenceModel(model=lda_gensim, corpus=corpus, coherence='u_mass')\n",
        "   coherence_lda = coherence_model_lda.get_coherence()\n",
        "   coherence_scores.append(coherence_lda)\n",
        "\n",
        "optimal_k = 2 + coherence_scores.index(max(coherence_scores))\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components=optimal_k, random_state=42)\n",
        "lda_model.fit(X)\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "topics = []\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "   top_features_ind = topic.argsort()[:-10 - 1:-1] # Top 10 features for each topic\n",
        "   topic_words = [feature_names[i] for i in top_features_ind]\n",
        "   topics.append(topic_words)\n",
        "\n",
        "for idx, topic in enumerate(topics):\n",
        "    print(f\"Topic {idx+1}: {topic}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47419df0-db75-49c7-8d4f-57b6a9040cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: was|the|is|it|this|acting|and|movie|weak|couldn\n",
            "Topic 2: is|superb|storyline|captivating|fantastic|absolutely|acting|this|and|the\n",
            "Topic 3: weak|film|terrible|stand|plot|couldn|was|this|and|acting\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LsiModel\n",
        "from gensim.matutils import Sparse2Corpus\n",
        "\n",
        "movie_reviews = [\n",
        "    \"This movie is absolutely fantastic! The acting is superb and the storyline is captivating.\",\n",
        "    \"I couldn't stand this film. The plot was weak, and the acting was terrible.\",\n",
        "    \"The movie was okay. It had some good moments but overall, it was disappointing.\"\n",
        "]\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(movie_reviews)\n",
        "\n",
        "\n",
        "corpus = Sparse2Corpus(X.T)\n",
        "\n",
        "\n",
        "coherence_scores = []\n",
        "for k in range(2, 11): # You can adjust the range for K\n",
        "   lsa_model = TruncatedSVD(n_components=k, random_state=42)\n",
        "   lsa_model.fit(X)\n",
        "   id2word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
        "   lsa_gensim = LsiModel(\n",
        "       corpus,\n",
        "       num_topics=k,\n",
        "       id2word=id2word,\n",
        "       chunksize=2000,\n",
        "       decay=0.5,\n",
        "       onepass=False\n",
        "   )\n",
        "   coherence_model_lsa = CoherenceModel(model=lsa_gensim, corpus=corpus, coherence='u_mass')\n",
        "   coherence_lsa = coherence_model_lsa.get_coherence()\n",
        "   coherence_scores.append(coherence_lsa)\n",
        "\n",
        "optimal_k = 2 + coherence_scores.index(max(coherence_scores))\n",
        "\n",
        "\n",
        "lsa_model = TruncatedSVD(n_components=optimal_k, random_state=42)\n",
        "lsa_model.fit(X)\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "topics = []\n",
        "for topic_idx, topic in enumerate(lsa_model.components_):\n",
        "   top_features_ind = topic.argsort()[:-10 - 1:-1] # Top 10 features for each topic\n",
        "   topic_words = [feature_names[i] for i in top_features_ind]\n",
        "   topics.append(topic_words)\n",
        "\n",
        "\n",
        "for idx, topic in enumerate(topics):\n",
        "   print(f\"Topic {idx + 1}: {'|'.join(topic)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4667251-f269-4bd1-eae8-e3766f4da2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: is|the|was|acting|and|this|weak|terrible|stand|plot\n",
            "Topic 2: it|was|moments|but|disappointing|good|some|had|overall|okay\n",
            "Topic 3: movie|was|the|superb|storyline|absolutely|captivating|fantastic|is|this\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.matutils import Sparse2Corpus\n",
        "movie_reviews = [\n",
        "    \"This movie is absolutely fantastic! The acting is superb and the storyline is captivating.\",\n",
        "    \"I couldn't stand this film. The plot was weak, and the acting was terrible.\",\n",
        "    \"The movie was okay. It had some good moments but overall, it was disappointing.\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(movie_reviews)  # Assuming 'df' is a list of documents\n",
        "\n",
        "corpus = Sparse2Corpus(X.T)\n",
        "\n",
        "coherence_scores = []\n",
        "for k in range(2, 11):  # You can adjust the range for K\n",
        "    lda_model = LatentDirichletAllocation(n_components=k, random_state=42)\n",
        "    lda_model.fit(X)\n",
        "    id2word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
        "    lda_gensim = LdaModel(\n",
        "        corpus,\n",
        "        num_topics=k,\n",
        "        id2word=id2word,\n",
        "        passes=15,\n",
        "        iterations=100,\n",
        "        random_state=42\n",
        "    )\n",
        "    coherence_model_lda = CoherenceModel(model=lda_gensim, corpus=corpus, coherence='u_mass')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append(coherence_lda)\n",
        "\n",
        "optimal_k = 3 + coherence_scores.index(max(coherence_scores))\n",
        "\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components=optimal_k, random_state=42)\n",
        "lda_model.fit(X)\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "topics = []\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    top_features_ind = topic.argsort()[:-10 - 1:-1]  # Top 10 features for each topic\n",
        "    topic_words = [feature_names[i] for i in top_features_ind]\n",
        "    topics.append(topic_words)\n",
        "\n",
        "\n",
        "for idx, topic in enumerate(topics):\n",
        "    print(f\"Topic {idx + 1}: {'|'.join(topic)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50818f0d-4f0d-40ea-882b-659ee1dcbbe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "   text = text.lower()\n",
        "   text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "   tokens = word_tokenize(text)\n",
        "   stop_words = set(stopwords.words('english'))\n",
        "   tokens = [word for word in tokens if word not in stop_words]\n",
        "   lemmatizer = WordNetLemmatizer()\n",
        "   tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "   preprocessed_text = ' '.join(tokens)\n",
        "   return preprocessed_text\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/code.csv')\n",
        "df\n",
        "preprocessed_df = [preprocess_text(doc) for doc in df]\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UL40vqD4izL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Wu4j7ToizGx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYEBnUMiizmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "5QX0n6rJ_P0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The \"best\" subject modeling algorithm is dependent on your particular data and objectives; there is no one \"best\" algorithm. Here's a summary of a few well-known algorithms to aid in your decision:\n",
        "\n",
        "1. LDA, or latent dirichlet allocation:\n",
        "\n",
        "\n",
        "Pros: Most reputable and extensively utilized. Effective in locating cohesive subjects with comprehensible word distributions.extend_more provides probabilistic results for the frequency of a topic in documents.extend_more\n",
        "Cons: May have trouble grasping brief materials or subjects that are quite similar.extend_more demands that the number of topics be predetermined, which can be difficult.\n",
        "\n",
        "2. Factorization of Non-Negative Matrix (NMF):\n",
        "\n",
        "Pros: Identifies parts-of-speech patterns with ease and manages sparse data well. Frequently quicker than LDA.extend_more\n",
        "Cons: Word meaning within subjects cannot be interpreted and results are not probabilistic.yell Not the best method for identifying themes.\n",
        "\n",
        "3. Latent Semantic Analysis with Probabilities (PLSA):\n",
        "\n",
        "Pros: Functions similarly to LDA, but frequently operates more quickly.extend_more useful for applications involving the decrease of dimensionality.\n",
        "Cons: Has some of the same restrictions as LDA, such as topic pre-specification.yell Maybe less successful at encapsulating intricate theme systems.yell\n",
        "\n",
        "4. BERTopic:\n",
        "\n",
        "Pros: This relatively new technique is excellent at finding brief, targeted subjects. frequently does well on user-generated content or writing on social networking.extend_more\n",
        "Cons: Compared to LDA or NMF, this approach is newer and has less established research. For larger papers or in-depth topic research, it might not be the best option.\n",
        "\n",
        "Here's a table summarizing the key points:\n",
        "\n",
        "Algorithm\tStrengths\tWeaknesses\n",
        "LDA\tEstablished, interpretable topics, probabilistic\tPre-specifying topics, struggles with short documents\n",
        "NMF\tHandles sparse data, fast\tNon-probabilistic, less interpretable topics\n",
        "PLSA\tFaster than LDA, dimensionality reduction\tSimilar limitations to LDA, less effective for complex topics\n",
        "BERTopic\tShort, focused topics, good for social media text\tNewer method, less established, may not be ideal for longer documents.\n",
        "\n",
        "It could be best to use PLSA or LDA for topical topics. For data that contain a lot of unnecessary phrases, NMF is helpful.\n",
        "Interpretability: LDA and PLSA are better options if you need to deduce a topic's meaning from the words used. NMF provides less interpretability and is not probabilistic.\n",
        "\n",
        "Speed: Generally speaking, NMF is quicker than LDA or PLSA, which is significant for very large datasets.\n",
        "\n",
        "In the end, the best method to select is to test many algorithms on your data to determine which one yields the most pertinent and understandable themes for your particular requirements.\n",
        "\n"
      ],
      "metadata": {
        "id": "OK34nZtojhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BERTopic: Because it has the best value of k and many topics as compaired to the others.\n",
        "\n",
        "Challenges Faced : Faced some difficulties while doing the excercise as some of them are new to me.\n",
        "\n",
        "Relevance : The models are relevance in NLP since they are being used in text analysis to generate topics and value of K."
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}