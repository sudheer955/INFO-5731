{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a937175a-3259-446d-d1a9-dbc9b9e4f887"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Scores:\n",
            "[0.77697842 0.77697842 0.71942446 0.69064748 0.70289855 0.71014493\n",
            " 0.74637681 0.73913043 0.6884058  0.66666667]\n",
            "Mean CV Accuracy: 0.7217651965384214\n",
            "\n",
            "Evaluation Metrics on Test Data:\n",
            "Accuracy: 0.729818780889621\n",
            "Precision: 0.6921658986175115\n",
            "Recall: 0.8261826182618262\n",
            "F1 Score: 0.7532597793380141\n",
            "Cross-Validation Scores:\n",
            "[0.82014388 0.76978417 0.73381295 0.71942446 0.79710145 0.77536232\n",
            " 0.7173913  0.74637681 0.68115942 0.66666667]\n",
            "Mean CV Accuracy: 0.7427223438640392\n",
            "\n",
            "Evaluation Metrics on Test Data (SVM):\n",
            "Accuracy: 0.7358594179022515\n",
            "Precision: 0.717479674796748\n",
            "Recall: 0.7766776677667767\n",
            "F1 Score: 0.745905969360803\n",
            "Cross-Validation Scores:\n",
            "[0.48920863 0.48201439 0.48201439 0.48201439 0.48550725 0.48550725\n",
            " 0.48550725 0.48550725 0.48550725 0.48550725]\n",
            "Mean CV Accuracy: 0.48482952768220205\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Metrics on Test Data (KNN):\n",
            "Accuracy: 0.500823723228995\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n",
            "Cross-Validation Scores:\n",
            "[0.65467626 0.63309353 0.60431655 0.60431655 0.63043478 0.65942029\n",
            " 0.62318841 0.64492754 0.57971014 0.58695652]\n",
            "Mean CV Accuracy: 0.6221040558857263\n",
            "\n",
            "Evaluation Metrics on Test Data (Decision Tree):\n",
            "Accuracy: 0.6320702910488742\n",
            "Precision: 0.6477132262051916\n",
            "Recall: 0.5764576457645765\n",
            "F1 Score: 0.610011641443539\n",
            "Cross-Validation Scores:\n",
            "[0.76258993 0.71223022 0.69784173 0.61870504 0.63768116 0.72463768\n",
            " 0.64492754 0.63043478 0.65942029 0.60144928]\n",
            "Mean CV Accuracy: 0.6689917631112501\n",
            "\n",
            "Evaluation Metrics on Test Data (Random Forest):\n",
            "Accuracy: 0.6836902800658978\n",
            "Precision: 0.7557603686635944\n",
            "Recall: 0.5412541254125413\n",
            "F1 Score: 0.6307692307692307\n",
            "Cross-Validation Scores:\n",
            "[0.71942446 0.63309353 0.66906475 0.64028777 0.65942029 0.70289855\n",
            " 0.65942029 0.67391304 0.63768116 0.55797101]\n",
            "Mean CV Accuracy: 0.655317485142321\n",
            "\n",
            "Evaluation Metrics on Test Data (XGBoost):\n",
            "Accuracy: 0.6490939044481054\n",
            "Precision: 0.6700251889168766\n",
            "Recall: 0.5852585258525853\n",
            "F1 Score: 0.6247798003523195\n",
            "Cross-Validation Scores:\n",
            "[0.52517986 0.5323741  0.54676259 0.5323741  0.50724638 0.52898551\n",
            " 0.52173913 0.53623188 0.52898551 0.46376812]\n",
            "Mean CV Accuracy: 0.5223647169221145\n",
            "\n",
            "Evaluation Metrics on Test Data (SVM with Word2Vec):\n",
            "Accuracy: 0.5096101043382757\n",
            "Precision: 0.504581901489118\n",
            "Recall: 0.9691969196919692\n",
            "F1 Score: 0.663653483992467\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Average Training Loss: 0.6114\n",
            "Epoch 1/3, Validation Loss: 0.5003, Validation Accuracy: 0.7659, Validation Precision: 0.7951, Validation Recall: 0.7349, Validation F1 Score: 0.7638\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "imdb_train = pd.read_csv('/content/stsa-train.txt', delimiter = '\\t', header = None)\n",
        "imdb_test = pd.read_csv('/content/stsa-test.txt', delimiter = '\\t', header = None)\n",
        "imdb_train.columns = ['original_data']\n",
        "imdb_test.columns = ['original_data']\n",
        "\n",
        "# Define a function to extract sentiment\n",
        "def extract_sentiment(text):\n",
        "    sentiment = re.match(r'^(\\d+)\\s', text).group(1)\n",
        "    return int(sentiment)\n",
        "\n",
        "# Extract sentiment and text\n",
        "imdb_train['sentiment'] = imdb_train['original_data'].apply(extract_sentiment)\n",
        "imdb_train['text'] = imdb_train['original_data'].apply(lambda x: re.sub(r'^\\d+\\s', '', x))\n",
        "\n",
        "# Extract sentiment and text\n",
        "imdb_test['sentiment'] = imdb_test['original_data'].apply(extract_sentiment)\n",
        "imdb_test['text'] = imdb_test['original_data'].apply(lambda x: re.sub(r'^\\d+\\s', '', x))\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into 80% train and 20% validation\n",
        "train_data, val_data = train_test_split(imdb_train, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation, special characters, and stop words\n",
        "    tokens = [ps.stem(word.lower()) for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    # Join the tokens back into a string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply preprocessing to the text column of train_data and val_data\n",
        "train_data['clean_text'] = train_data['text'].apply(preprocess_text)\n",
        "val_data['clean_text'] = val_data['text'].apply(preprocess_text)\n",
        "imdb_test['clean_text'] = imdb_test['text'].apply(preprocess_text)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(train_data['clean_text'])\n",
        "\n",
        "# Transform the validation and test data\n",
        "X_val_tfidf = vectorizer.transform(val_data['clean_text'])\n",
        "X_test_tfidf = vectorizer.transform(imdb_test['clean_text'])\n",
        "\n",
        "\n",
        "# Initialize Multinomial Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Perform 10-fold cross-validation on the validation data\n",
        "cv_scores = cross_val_score(nb_model, X_val_tfidf, val_data['sentiment'], cv=10, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-Validation Scores:\")\n",
        "print(cv_scores)\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
        "\n",
        "# Train the model on the entire validation data\n",
        "nb_model.fit(X_val_tfidf, val_data['sentiment'])\n",
        "\n",
        "# Predict sentiment on the test data\n",
        "test_predictions = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(imdb_test['sentiment'], test_predictions)\n",
        "precision = precision_score(imdb_test['sentiment'], test_predictions)\n",
        "recall = recall_score(imdb_test['sentiment'], test_predictions)\n",
        "f1 = f1_score(imdb_test['sentiment'], test_predictions)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nEvaluation Metrics on Test Data:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "f1\n",
        "# Initialize Support Vector Machine model\n",
        "svm_model = SVC()\n",
        "\n",
        "# Perform 10-fold cross-validation on the validation data\n",
        "cv_scores_svm = cross_val_score(svm_model, X_val_tfidf, val_data['sentiment'], cv=10, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-Validation Scores:\")\n",
        "print(cv_scores_svm)\n",
        "print(\"Mean CV Accuracy:\", cv_scores_svm.mean())\n",
        "\n",
        "# Train the model on the entire validation data\n",
        "svm_model.fit(X_val_tfidf, val_data['sentiment'])\n",
        "\n",
        "# Predict sentiment on the test data\n",
        "test_predictions_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy_svm = accuracy_score(imdb_test['sentiment'], test_predictions_svm)\n",
        "precision_svm = precision_score(imdb_test['sentiment'], test_predictions_svm)\n",
        "recall_svm = recall_score(imdb_test['sentiment'], test_predictions_svm)\n",
        "f1_svm = f1_score(imdb_test['sentiment'], test_predictions_svm)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nEvaluation Metrics on Test Data (SVM):\")\n",
        "print(\"Accuracy:\", accuracy_svm)\n",
        "print(\"Precision:\", precision_svm)\n",
        "print(\"Recall:\", recall_svm)\n",
        "print(\"F1 Score:\", f1_svm)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize K-Nearest Neighbors model\n",
        "knn_model = KNeighborsClassifier()\n",
        "\n",
        "# Perform 10-fold cross-validation on the validation data\n",
        "cv_scores_knn = cross_val_score(knn_model, X_val_tfidf, val_data['sentiment'], cv=10, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-Validation Scores:\")\n",
        "print(cv_scores_knn)\n",
        "print(\"Mean CV Accuracy:\", cv_scores_knn.mean())\n",
        "\n",
        "# Train the model on the entire validation data\n",
        "knn_model.fit(X_val_tfidf, val_data['sentiment'])\n",
        "\n",
        "# Predict sentiment on the test data\n",
        "test_predictions_knn = knn_model.predict(X_test_tfidf)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy_knn = accuracy_score(imdb_test['sentiment'], test_predictions_knn)\n",
        "precision_knn = precision_score(imdb_test['sentiment'], test_predictions_knn)\n",
        "recall_knn = recall_score(imdb_test['sentiment'], test_predictions_knn)\n",
        "f1_knn = f1_score(imdb_test['sentiment'], test_predictions_knn)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nEvaluation Metrics on Test Data (KNN):\")\n",
        "print(\"Accuracy:\", accuracy_knn)\n",
        "print(\"Precision:\", precision_knn)\n",
        "print(\"Recall:\", recall_knn)\n",
        "print(\"F1 Score:\", f1_knn)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize Decision Tree model\n",
        "dt_model = DecisionTreeClassifier()\n",
        "\n",
        "# Perform 10-fold cross-validation on the validation data\n",
        "cv_scores_dt = cross_val_score(dt_model, X_val_tfidf, val_data['sentiment'], cv=10, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-Validation Scores:\")\n",
        "print(cv_scores_dt)\n",
        "print(\"Mean CV Accuracy:\", cv_scores_dt.mean())\n",
        "\n",
        "# Train the model on the entire validation data\n",
        "dt_model.fit(X_val_tfidf, val_data['sentiment'])\n",
        "\n",
        "# Predict sentiment on the test data\n",
        "test_predictions_dt = dt_model.predict(X_test_tfidf)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy_dt = accuracy_score(imdb_test['sentiment'], test_predictions_dt)\n",
        "precision_dt = precision_score(imdb_test['sentiment'], test_predictions_dt)\n",
        "recall_dt = recall_score(imdb_test['sentiment'], test_predictions_dt)\n",
        "f1_dt = f1_score(imdb_test['sentiment'], test_predictions_dt)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nEvaluation Metrics on Test Data (Decision Tree):\")\n",
        "print(\"Accuracy:\", accuracy_dt)\n",
        "print(\"Precision:\", precision_dt)\n",
        "print(\"Recall:\", recall_dt)\n",
        "print(\"F1 Score:\", f1_dt)\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Perform 10-fold cross-validation on the validation data\n",
        "cv_scores_rf = cross_val_score(rf_model, X_val_tfidf, val_data['sentiment'], cv=10, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-Validation Scores:\")\n",
        "print(cv_scores_rf)\n",
        "print(\"Mean CV Accuracy:\", cv_scores_rf.mean())\n",
        "\n",
        "# Train the model on the entire validation data\n",
        "rf_model.fit(X_val_tfidf, val_data['sentiment'])\n",
        "\n",
        "# Predict sentiment on the test data\n",
        "test_predictions_rf = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy_rf = accuracy_score(imdb_test['sentiment'], test_predictions_rf)\n",
        "precision_rf = precision_score(imdb_test['sentiment'], test_predictions_rf)\n",
        "recall_rf = recall_score(imdb_test['sentiment'], test_predictions_rf)\n",
        "f1_rf = f1_score(imdb_test['sentiment'], test_predictions_rf)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nEvaluation Metrics on Test Data (Random Forest):\")\n",
        "print(\"Accuracy:\", accuracy_rf)\n",
        "print(\"Precision:\", precision_rf)\n",
        "print(\"Recall:\", recall_rf)\n",
        "print(\"F1 Score:\", f1_rf)\n",
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# Initialize XGBoost model\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "# Perform 10-fold cross-validation on the validation data\n",
        "cv_scores_xgb = cross_val_score(xgb_model, X_val_tfidf, val_data['sentiment'], cv=10, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-Validation Scores:\")\n",
        "print(cv_scores_xgb)\n",
        "print(\"Mean CV Accuracy:\", cv_scores_xgb.mean())\n",
        "\n",
        "# Train the model on the entire validation data\n",
        "xgb_model.fit(X_val_tfidf, val_data['sentiment'])\n",
        "\n",
        "# Predict sentiment on the test data\n",
        "test_predictions_xgb = xgb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy_xgb = accuracy_score(imdb_test['sentiment'], test_predictions_xgb)\n",
        "precision_xgb = precision_score(imdb_test['sentiment'], test_predictions_xgb)\n",
        "recall_xgb = recall_score(imdb_test['sentiment'], test_predictions_xgb)\n",
        "f1_xgb = f1_score(imdb_test['sentiment'], test_predictions_xgb)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nEvaluation Metrics on Test Data (XGBoost):\")\n",
        "print(\"Accuracy:\", accuracy_xgb)\n",
        "print(\"Precision:\", precision_xgb)\n",
        "print(\"Recall:\", recall_xgb)\n",
        "print(\"F1 Score:\", f1_xgb)\n",
        "\n",
        "# Define a custom transformer to convert documents into average Word2Vec vectors\n",
        "class AverageWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.vector_size = model.vector_size\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.model.wv[word] for word in document.split() if word in self.model.wv] or [np.zeros(self.vector_size)], axis=0)\n",
        "            for document in X\n",
        "        ])\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=train_data['clean_text'].apply(str.split), vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Convert documents to Word2Vec vectors\n",
        "vectorizer = AverageWord2VecVectorizer(word2vec_model)\n",
        "X_train_word2vec = vectorizer.transform(train_data['clean_text'])\n",
        "X_val_word2vec = vectorizer.transform(val_data['clean_text'])\n",
        "X_test_word2vec = vectorizer.transform(imdb_test['clean_text'])\n",
        "\n",
        "# Train SVM model using Word2Vec vectors\n",
        "svm_model_word2vec = SVC()\n",
        "\n",
        "# Perform 10-fold cross-validation on the validation data\n",
        "cv_scores_svm_word2vec = cross_val_score(svm_model_word2vec, X_val_word2vec, val_data['sentiment'], cv=10, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-Validation Scores:\")\n",
        "print(cv_scores_svm_word2vec)\n",
        "print(\"Mean CV Accuracy:\", cv_scores_svm_word2vec.mean())\n",
        "\n",
        "# Train the model on the entire validation data\n",
        "svm_model_word2vec.fit(X_val_word2vec, val_data['sentiment'])\n",
        "\n",
        "# Predict sentiment on the test data\n",
        "test_predictions_svm_word2vec = svm_model_word2vec.predict(X_test_word2vec)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy_svm_word2vec = accuracy_score(imdb_test['sentiment'], test_predictions_svm_word2vec)\n",
        "precision_svm_word2vec = precision_score(imdb_test['sentiment'], test_predictions_svm_word2vec)\n",
        "recall_svm_word2vec = recall_score(imdb_test['sentiment'], test_predictions_svm_word2vec)\n",
        "f1_svm_word2vec = f1_score(imdb_test['sentiment'], test_predictions_svm_word2vec)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nEvaluation Metrics on Test Data (SVM with Word2Vec):\")\n",
        "print(\"Accuracy:\", accuracy_svm_word2vec)\n",
        "print(\"Precision:\", precision_svm_word2vec)\n",
        "print(\"Recall:\", recall_svm_word2vec)\n",
        "print(\"F1 Score:\", f1_svm_word2vec)\n",
        "\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize text data\n",
        "def tokenize_text(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "# Encode training, validation, and test data\n",
        "train_encoded = [tokenize_text(text) for text in train_data['clean_text']]\n",
        "val_encoded = [tokenize_text(text) for text in val_data['clean_text']]\n",
        "test_encoded = [tokenize_text(text) for text in imdb_test['clean_text']]\n",
        "\n",
        "# Create DataLoader for training, validation, and test data\n",
        "train_dataset = TensorDataset(torch.cat([x['input_ids'] for x in train_encoded]),\n",
        "                              torch.cat([x['attention_mask'] for x in train_encoded]),\n",
        "                              torch.tensor(train_data['sentiment'].values))\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(torch.cat([x['input_ids'] for x in val_encoded]),\n",
        "                            torch.cat([x['attention_mask'] for x in val_encoded]),\n",
        "                            torch.tensor(val_data['sentiment'].values))\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "test_dataset = TensorDataset(torch.cat([x['input_ids'] for x in test_encoded]),\n",
        "                             torch.cat([x['attention_mask'] for x in test_encoded]),\n",
        "                             torch.tensor(imdb_test['sentiment'].values))\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Fine-tune the pre-trained BERT model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "def train(model, optimizer, train_loader, val_loader, epochs=3):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for input_ids, attention_mask, labels in train_loader:\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Average Training Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "        # Evaluate on validation data\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, labels in val_loader:\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                val_loss += loss.item()\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "            val_precision = precision_score(val_labels, val_preds)\n",
        "            val_recall = recall_score(val_labels, val_preds)\n",
        "            val_f1 = f1_score(val_labels, val_preds)\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}, '\n",
        "                  f'Validation Accuracy: {val_accuracy:.4f}, '\n",
        "                  f'Validation Precision: {val_precision:.4f}, '\n",
        "                  f'Validation Recall: {val_recall:.4f}, '\n",
        "                  f'Validation F1 Score: {val_f1:.4f}')\n",
        "\n",
        "# Train the model\n",
        "train(model, optimizer, train_loader, val_loader)\n",
        "\n",
        "# Evaluate on test data\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, labels in test_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy_bert = accuracy_score(test_labels, test_preds)\n",
        "precision_bert = precision_score(test_labels, test_preds)\n",
        "recall_bert = recall_score(test_labels, test_preds)\n",
        "f1_bert = f1_score(test_labels, test_preds)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nEvaluation Metrics on Test Data (BERT):\")\n",
        "print(\"Accuracy:\", accuracy_bert)\n",
        "print(\"Precision:\", precision_bert)\n",
        "print(\"Recall:\", recall_bert)\n",
        "print(\"F1 Score:\", f1_bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data set mention in the assignment was too long and It was taking alot of time to preprrocess and clean it first and then using it for clustering was even more time consuming. So instead, I download a dataset from https://github.com/PawanKrGunjan/Natural-Language-Processing/blob/main/Sarcasm%20Detection/sarcasm.json. This is for sarcasm in which 1 indicates sarcastic and 0 indicates not sarcastic."
      ],
      "metadata": {
        "id": "1eDPlY6YbeAQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ba708a-8799-4d26-eb24-f2a8a33c723f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results:\n",
            "K-means - ARI: -0.00887055695096076 Silhouette Score: 0.0011449686543569446\n",
            "Hierarchical - ARI: -0.00038797454305711124 Silhouette Score: 0.0003877639781810512\n",
            "K-means with Word2Vec - ARI: -0.005723980609364586 Silhouette Score: 0.979387477786733\n",
            "K-means with BERT - ARI: 5.787865645758347e-05 Silhouette Score: 0.04185712\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_json('/content/sarcasm (2).json')\n",
        "data = data[['headline', 'is_sarcastic']]\n",
        "data.head()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Join tokens back into a string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply preprocessing to the headline column\n",
        "data['clean_text'] = data['headline'].apply(preprocess_text)\n",
        "\n",
        "data['is_sarcastic'].value_counts()\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(data['clean_text'])\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Apply K-means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_tfidf)\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X_tfidf)\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, cut_tree\n",
        "\n",
        "# Compute pairwise distances and perform hierarchical clustering\n",
        " linkage_matrix = linkage(X_tfidf.toarray(), method='ward')\n",
        "\n",
        "# Extract cluster assignments\n",
        "cut_tree_labels = cut_tree(linkage_matrix, n_clusters=2).flatten()\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "# Define a custom transformer to convert documents into average Word2Vec vectors\n",
        "class AverageWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.vector_size = model.vector_size\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.model.wv[word] for word in document.split() if word in self.model.wv] or [np.zeros(self.vector_size)], axis=0)\n",
        "            for document in X\n",
        "        ])\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=data['clean_text'], vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Convert documents to Word2Vec vectors\n",
        "vectorizer = AverageWord2VecVectorizer(word2vec_model)\n",
        "X_word2vec = vectorizer.transform(data['clean_text'])\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Train KMeans clustering using Word2Vec vectors\n",
        "kmeans_word2vec = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans_labels_word2vec = kmeans_word2vec.fit_predict(X_word2vec)\n",
        "\n",
        "## Using BERT\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize text data\n",
        "tokenized_data = data['clean_text'].apply(lambda x: tokenizer(x, return_tensors='pt', padding=True, truncation=True))\n",
        "\n",
        "# Encode tokenized text into numerical representations\n",
        "encoded_data = []\n",
        "for i in range(len(tokenized_data)):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_data[i])\n",
        "        encoded_data.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
        "X_bert = np.vstack(encoded_data)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Apply K-means clustering using BERT embeddings\n",
        "kmeans_bert = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans_labels_bert = kmeans_bert.fit_predict(X_bert)\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
        "from scipy.cluster.hierarchy import linkage, cut_tree\n",
        "\n",
        "# Ground truth labels\n",
        "true_labels = data['is_sarcastic'].values\n",
        "\n",
        "# Evaluate K-means clustering\n",
        "ari_kmeans = adjusted_rand_score(true_labels, kmeans_labels)\n",
        "silhouette_kmeans = silhouette_score(X_tfidf, kmeans_labels)\n",
        "\n",
        "# Evaluate DBSCAN clustering\n",
        "#ari_dbscan = adjusted_rand_score(true_labels, dbscan_labels)\n",
        "#silhouette_dbscan = silhouette_score(X_tfidf, dbscan_labels)\n",
        "\n",
        "# Evaluate hierarchical clustering\n",
        "ari_hierarchical = adjusted_rand_score(true_labels, cut_tree_labels)\n",
        "silhouette_hierarchical = silhouette_score(X_tfidf, cut_tree_labels)\n",
        "\n",
        "# Evaluate K-means clustering with Word2Vec\n",
        "ari_kmeans_word2vec = adjusted_rand_score(true_labels, kmeans_labels_word2vec)\n",
        "silhouette_kmeans_word2vec = silhouette_score(X_word2vec, kmeans_labels_word2vec)\n",
        "\n",
        "# Evaluate K-means clustering with BERT embeddings\n",
        "ari_kmeans_bert = adjusted_rand_score(true_labels, kmeans_labels_bert)\n",
        "silhouette_kmeans_bert = silhouette_score(X_bert, kmeans_labels_bert)\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"Evaluation Results:\")\n",
        "print(\"K-means - ARI:\", ari_kmeans, \"Silhouette Score:\", silhouette_kmeans)\n",
        "#print(\"DBSCAN - ARI:\", ari_dbscan, \"Silhouette Score:\", silhouette_dbscan)\n",
        "print(\"Hierarchical - ARI:\", ari_hierarchical, \"Silhouette Score:\", silhouette_hierarchical)\n",
        "print(\"K-means with Word2Vec - ARI:\", ari_kmeans_word2vec, \"Silhouette Score:\", silhouette_kmeans_word2vec)\n",
        "print(\"K-means with BERT - ARI:\", ari_kmeans_bert, \"Silhouette Score:\", silhouette_kmeans_bert)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your response here:**\n",
        "\n",
        "The evaluation results show that K-means, DBSCAN, and Hierarchical clustering algorithms performed poorly in terms of adjusted Rand index (ARI) and silhouette score. K-means achieved the highest silhouette score among these traditional clustering algorithms, but it still yielded a very low ARI, indicating poor agreement with the ground truth labels. DBSCAN and Hierarchical clustering performed even worse, with negative ARI values, indicating clustering results worse than random labeling. On the other hand, both Word2Vec and BERT embeddings used with K-means achieved higher silhouette scores, indicating better separation of clusters in the embedding space. However, the ARI values for Word2Vec and BERT embeddings were still low, suggesting limited agreement with the ground truth labels. Overall, while traditional clustering algorithms struggled to capture meaningful clusters in the data, embedding-based approaches showed promise in improving clustering performance, but further optimization and tuning may be required to achieve better agreement with the ground truth labels.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exercises provided in this assignment were quite comprehensive and covered various aspects of text analysis and machine learning techniques. Here are some reflective feedback points:\n",
        "\n",
        "Diverse Coverage: The exercises covered a wide range of machine learning algorithms for text classification and clustering tasks, including traditional methods like Naive Bayes, SVM, KNN, Decision Trees, Random Forest, and XGBoost, as well as advanced techniques like Word2Vec and BERT embeddings. This allowed for a thorough exploration of different approaches and their performance in different scenarios.\n",
        "\n",
        "Hands-on Practice: The exercises provided hands-on practice with implementing machine learning algorithms using Python libraries such as scikit-learn, Gensim, and Hugging Face Transformers. This practical experience was valuable in reinforcing theoretical concepts and improving coding skills.\n",
        "\n",
        "Evaluation and Performance Metrics: The exercises emphasized the importance of evaluation and performance metrics in assessing the quality of machine learning models. Metrics such as accuracy, precision, recall, F1-score, adjusted Rand index, and silhouette score were used to evaluate the models, providing a comprehensive understanding of their strengths and limitations.\n",
        "\n",
        "Integration of External Libraries: Integration of external libraries like Gensim for Word2Vec embeddings and Hugging Face Transformers for BERT embeddings added depth to the exercises and exposed learners to state-of-the-art tools and techniques in natural language processing.\n",
        "\n",
        "Clear Instructions: The instructions provided for each exercise were clear and well-structured, making it easy to follow along and complete the tasks within a reasonable timeframe."
      ],
      "metadata": {
        "id": "0PJKvFktqSCd"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}